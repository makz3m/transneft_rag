# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

**Authors:** Patrick Lewis†‡, Ethan Perez?, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†
†Facebook AI Research; ‡University College London; ?New York University; plewis@fb.com

## Abstract
Large pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.

## 1 Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data. They can do so without any access to an external memory, as a parameterized implicit knowledge base. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations.” Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM and ORQA, two recently introduced models that combine masked language models with a differentiable retriever, have shown promising results.The middle ear is defined as the part of the ear that includes the tympanic cavity and the three ossicles. It plays a crucial role in the process of hearing by transmitting sound vibrations from the outer ear to the inner ear. The middle ear is located between the outer ear and the inner ear and is connected to the throat via the Eustachian tube, which helps to equalize pressure on both sides of the eardrum.The text discusses two models, RAG-Sequence and RAG-Token, which are designed for generating text based on retrieved documents.

### 2.1 Models

**RAG-Sequence Model**:
- This model uses the same retrieved document to generate the entire output sequence.
- It treats the retrieved document as a single latent variable, which is marginalized to compute the sequence-to-sequence probability \( p(y|x) \) using a top-K approximation.
- The top K documents are retrieved, and the generator computes the output sequence probability for each document, which are then combined.

**RAG-Token Model**:
- In contrast, the RAG-Token model allows for the selection of different latent documents for each target token.
- This flexibility enables the generator to draw content from multiple documents when generating an answer.
- Similar to RAG-Sequence, it retrieves the top K documents, but it produces a distribution for each output token based on the selected documents.

Both models can be adapted for sequence classification tasks by treating the target class as a sequence of length one, making them equivalent in that context.

### 2.2 Retriever: DPR
The retrieval component is based on Dense Passage Retrieval (DPR), which employs a bi-encoder architecture.
- The document representation \( d(z) \) is generated using a BERT-based encoder, while the query representation \( q(x) \) is also derived from a BERT-based model.
- The retrieval process involves solving a Maximum Inner Product Search (MIPS) problem to identify the top K documents with the highest prior probability.

### 2.3 Generator: BART
The generator component can utilize any encoder-decoder architecture, but the authors chose BART-large, a pre-trained seq2seq transformer model.
- The input and retrieved content are concatenated for generation.
- BART has been pre-trained with a denoising objective and has shown superior performance on various generation tasks compared to similarly sized models.

### 2.4 Training
The retriever and generator are jointly trained without direct supervision on the specific documents to retrieve. The training involves a corpus of input/output pairs, allowing the models to learn from the data effectively.The text discusses the training and decoding processes of a model called RAG (Retrieval-Augmented Generation) for knowledge-intensive tasks, particularly focusing on open-domain question answering and abstractive question answering.

### Training Process
- The model minimizes the negative marginal log-likelihood of each target using stochastic gradient descent with the Adam optimizer.
- The document encoder (BERTd) is kept fixed during training, while the query encoder (BERTq) and the BART generator are fine-tuned.
- The document index is not updated during training, which is a departure from the REALM approach.

### Decoding Methods
Two decoding methods are described for RAG:

1. **RAG-Token**:
- This model operates as a standard autoregressive sequence-to-sequence generator.
- It uses a transition probability to decode and can be integrated into a standard beam decoder.

2. **RAG-Sequence**:
- This method does not allow for a conventional per-token likelihood, necessitating a different approach.
- It involves running beam search for each document and scoring hypotheses based on the likelihood of the generated sequences.
- Two decoding procedures are introduced:
- **Thorough Decoding**: Involves multiple forward passes for each document to estimate probabilities.
- **Fast Decoding**: Makes approximations to avoid additional forward passes, improving efficiency.

### Experimental Setup
- The experiments utilize a single Wikipedia dump as the knowledge source, specifically the December 2018 version, split into 100-word chunks, resulting in 21 million documents.
- The model is evaluated on various open-domain question answering datasets, including Natural Questions, TriviaQA, WebQuestions, and CuratedTrec.
- RAG is compared against extractive QA paradigms and closed-book QA approaches, with performance measured using Exact Match (EM) scores.

### Abstractive Question Answering
- RAG is also tested for its ability to generate free-form answers in an abstractive manner using the MSMARCO NLG task.
- The task involves questions and answers without relying on the provided passages, focusing solely on the generation capabilities of the model.

This overview highlights the key components of the RAG model's training, decoding strategies, and experimental evaluations in the context of knowledge-intensive tasks.The text discusses the capabilities of the RAG (Retrieval-Augmented Generation) model in various tasks, particularly focusing on open-domain question answering, Jeopardy question generation, and fact verification.

In the context of open-domain question answering, the text highlights that some questions in the MSMARCO dataset cannot be answered accurately without access to the original passages, indicating a limitation in performance when relying solely on the model's internal knowledge. It also notes that certain questions cannot be answered using Wikipedia alone, suggesting that RAG's ability to leverage parametric knowledge is beneficial for generating reasonable responses.

The section on Jeopardy question generation emphasizes the complexity of generating questions that require guessing an entity based on a factual statement. The authors propose using a BART model for comparison and evaluate the generated questions using the Q-BLEU-1 metric, which is designed to better correlate with human judgment in question generation tasks. Human evaluations are also conducted to assess the factuality and specificity of the generated questions.

In the fact verification task, the FEVER dataset is utilized to classify claims based on their support or refutation by Wikipedia. The authors describe their approach of mapping FEVER class labels to output tokens and training the model without supervision on retrieved evidence, which is a departure from many existing methods. They explore both a three-way and a two-way classification task, reporting label accuracy for both.

The results section indicates that RAG achieves state-of-the-art performance across various open-domain QA tasks, combining the strengths of both generation and retrieval-based approaches. The model's ability to generate answers from documents that contain clues but not the exact answer is highlighted as a significant advantage over traditional extractive methods.The text provided contains results and discussions from a scientific article focused on various models for question answering and text generation tasks. Here’s a summary of the key points:

1. **Open-Domain QA Test Scores**: The article presents test scores for different models on various datasets, including NQ (Natural Questions), TQA (Trivia Question Answering), and WQ (Web Questions). The models compared include T5, REALM, DPR, and RAG (Retrieval-Augmented Generation) variants.

2. **Abstractive Question Answering**: RAG-Sequence outperforms BART in generating answers for the Open MS-MARCO NLG task. The performance is notable given that many questions require access to specific gold passages, which RAG does not utilize.

3. **Jeopardy Question Generation**: RAG-Token shows superior performance in generating Jeopardy questions compared to RAG-Sequence and BART. Human evaluations indicate that RAG is more factual and specific in its outputs. The model's ability to combine information from multiple documents contributes to its effectiveness.

4. **Fact Verification**: RAG's performance in fact verification tasks is competitive with state-of-the-art models, despite not relying on complex pipeline systems or intermediate retrieval supervision.

Overall, the findings suggest that RAG models demonstrate strong capabilities in both generating and verifying information, outperforming traditional models in several aspects.The text appears to be an excerpt from a scientific article discussing the performance of RAG (Retrieval-Augmented Generation) models in various tasks, particularly in comparison to BART and other models. It highlights the effectiveness of RAG in generating factually accurate responses, its retrieval mechanism, and the diversity of its outputs.

Key points include:

1. **Document References**: The text references classic works of American literature, specifically mentioning Ernest Hemingway's novels "A Farewell to Arms" and "The Sun Also Rises," indicating the context in which these works are discussed.

2. **Model Performance**: The article compares the performance of RAG models against BART in generating responses to various tasks, including defining terms and answering questions. It notes that RAG models produce more specific and factually accurate responses.

3. **Retrieval Mechanism**: The effectiveness of RAG's retrieval mechanism is assessed through ablation studies, showing that learned retrieval improves results across tasks.

4. **Diversity in Generation**: The article discusses the diversity of generated responses, indicating that RAG models produce a higher ratio of distinct n-grams compared to BART.

5. **Index Hot-Swapping**: The ability of RAG to update its knowledge base at test time is highlighted as an advantage over parametric models, which require retraining to adapt to new information.

Overall, the text emphasizes the strengths of RAG models in generating accurate and diverse responses while also discussing their retrieval capabilities.The provided text appears to be excerpts from a scientific article discussing various models and their performance in question generation and retrieval tasks. The tables summarize human assessments and model evaluations, including metrics like Exact Match, Factuality, and various scoring methods (e.g., B-1, Rouge-L).

Key points include:

1. **Human Assessments**: The tables present results from the Jeopardy Question Generation Task, comparing different models (BART, RAG) based on their performance in terms of exact match and factuality.

2. **Model Performance**: The ablation studies show the performance of different RAG models across various tasks, indicating how retrieval methods impact accuracy and recall.

3. **Document Retrieval**: The effect of retrieving more documents is analyzed, showing that while RAG-Sequence benefits from more documents, RAG-Token performance peaks at a certain point.

4. **Related Work**: The article references prior research that highlights the importance of retrieval in improving performance across various NLP tasks, suggesting that a unified retrieval-based architecture can be effective for multiple applications.

Overall, the text emphasizes the significance of retrieval mechanisms in enhancing the performance of question generation and answering systems.# General-Purpose Architectures for NLP

Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks after fine-tuning. GPT-2 later demonstrated that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART and T5 propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture by learning a retrieval module to augment pre-trained, generative language models.

## Learned Retrieval

There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models similar to ours. Some work optimizes the retrieval module to aid in a specific downstream task such as question answering, using search, reinforcement learning, or a latent variable approach as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.

## Memory-based Architectures

Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks. Concurrent work learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings. A key feature of our memory is that it is comprised of raw text rather than distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval.

## Retrieve-and-Edit Approaches

Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation and Semantic Parsing. Our approach does have several differences, including less emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval and retrieving evidence documents rather than related training pairs. That said, RAG techniques may work well in these settings and could represent promising future work.

## Discussion

In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state-of-the-art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted a thorough investigation of the learned retrieval component, validating its effectiveness, and illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some other objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.**Broader Impact**

This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.

With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content. Advanced language models may also lead to the automation of various jobs in the coming decades. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.

**Acknowledgments**

The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.The text provided appears to be a list of references from a scientific article, specifically related to advancements in natural language processing and machine learning. Each entry includes the authors, title of the work, publication details, and URLs for accessing the papers. If you need assistance with a specific aspect of this content, such as summarizing the findings or discussing the implications of the research, please let me know!The text provided appears to be a list of references from a scientific article, specifically related to advancements in language models, retrieval-augmented techniques, and question answering systems. Each entry includes the authors, title, publication venue, and a URL for accessing the paper.

If you need assistance with a specific aspect of this content, such as summarizing the findings of a particular paper or discussing the implications of the research, please let me know!The text provided appears to be a list of references from a scientific article, specifically in the field of computational linguistics and natural language processing. It includes citations for various papers, including their authors, titles, publication venues, and URLs for accessing the papers.

If you need assistance with a specific aspect of this text, such as summarizing the content, extracting certain information, or analyzing the references, please let me know!The text provided appears to be a list of references from a scientific article, specifically related to natural language processing and machine learning. It includes citations for various papers, conference proceedings, and preprints, detailing authors, titles, publication venues, and URLs for accessing the documents.

If you need assistance with a specific aspect of this content, such as summarizing the findings of a particular paper or discussing the implications of the research, please let me know!The text provided appears to be a list of references from a scientific article, specifically related to natural language processing and artificial intelligence. Each entry includes the authors, title of the work, publication details, and URLs for accessing the papers. If you need assistance with a specific aspect of this content, such as summarizing the findings of a particular paper or discussing the significance of the research, please let me know!The text provided appears to be a list of references from a scientific article, specifically related to natural language processing and question generation. It includes citations for works by Thomas Wolf et al., Shiyue Zhang and Mohit Bansal, and Wanjun Zhong et al. Each reference includes details such as the title of the work, the conference or journal in which it was published, and relevant URLs or DOIs for accessing the papers.

If you need further assistance or a summary of the content, please let me know!# Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

## A. Implementation Details
For Open-domain QA, we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four and the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.

## B. Human Evaluation
The human evaluation process involves annotators determining which of two sentences is more factually true regarding a given subject. Annotators are encouraged to use the internet to verify the truthfulness of the sentences. The evaluation interface includes control questions to ensure accuracy, and annotators are provided with detailed instructions and examples. To mitigate biases, the assignment of sentences A and B is randomized for each example. Some gold sentences are included to assess annotator accuracy, and annotations from underperforming annotators are excluded from the results.

## C. Training Setup Details
All RAG models and BART baselines are trained using Fairseq. Training is conducted with mixed precision floating point arithmetic, distributed across 8, 32GB NVIDIA V100 GPUs, although training and inference can also be performed on a single GPU. Maximum Inner Product Search with FAISS is utilized on CPU for efficiency, with document index vectors stored on CPU, requiring approximately 100GB of CPU memory for all of Wikipedia. After submission, the code has been ported to HuggingFace, and the implementation is open-sourced. The document index is compressed using FAISS’s tools, reducing the CPU memory requirement to 36GB. Scripts for running experiments with RAG are available on GitHub, along with an interactive demo of a RAG model.# Further Details on Open-Domain QA

For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training, as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in the top 1000 documents for the query.

## CuratedTrec Preprocessing

The answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models. To overcome this, we use a preprocessing step where we first retrieve the top 1000 documents for each query and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with whitespace.

## TriviaQA Evaluation Setups

The open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the dataset splits used in DPR, which are consistent with common practice in open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. used the TriviaQA official Wikipedia test set instead. Févry et al. follow this convention in order to compare with Roberts et al. We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.

# Further Details on FEVER

For FEVER classification, we follow the practice from previous work and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either "Supported", "Refuted" or "Not Enough Info", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump than we do, directly tackling this task is not straightforward. We hope to address this in future work.

# Null Document Probabilities

We experimented with adding a "Null document" mechanism to RAG, similar to REALM, in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally "retrieve" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modeling this null document logit by learning (i) a document embedding for the null document, (ii) a static learned bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.

# Parameters

Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, making a total of 626M trainable parameters.The text provided includes a table summarizing the number of instances in various datasets used for different tasks in open-domain question answering (QA). It also discusses the performance of different models, particularly focusing on the T5 model and its variants, as well as the challenges faced in retrieval components during preliminary experiments.

### Summary of Key Points:

1. **Dataset Instances**: The table lists the number of training, development, and test instances for several datasets, including Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, and FEVER (both 2-way and 3-way).

2. **Model Performance**:
- The T5-11B model, with 11 billion parameters, is noted as the best performing "closed-book" open-domain QA model.
- T5-large (770M parameters) achieves a score of 28.9 Exact Match (EM) on Natural Questions, which is significantly lower than the 44.5 EM achieved by the RAG-Sequence model.
- The text suggests that hybrid models (parametric/non-parametric) can achieve strong performance with fewer trainable parameters.

3. **Memory Index**: The non-parametric memory index consists of 21 million 728-dimensional vectors, totaling 15.3 billion values, which can be stored efficiently.

4. **Retrieval Collapse**:
- Preliminary experiments indicated that the retrieval component could "collapse," leading to the retrieval of the same documents regardless of input, particularly in tasks like story generation.
- This collapse may result from the nature of the task or the length of target sequences, which can affect the informativeness of gradients for the retriever.
- Previous research (Perez et al.) also noted issues with spurious retrieval results when optimizing retrieval components for downstream task performance.

This summary encapsulates the main findings and observations from the provided text, focusing on dataset sizes, model performance, and challenges in retrieval mechanisms.