# RAG система и ее оптимизация

## Формулировка задачи

Есть набор файлов - научные статьи с сайта arxiv.org. Необходимо создать систему, которая парсит эти файлы для векторной базы данных для IR-системы в составе чат-бота.

## Бейзлайн

### Метрика

Для метрики будем использовать синтетический набор данных, состоящий из пар "вопрос - эталонный ответ". Будем просить llm сравнивать ответ нашей модели с эталонным решением и оценивать по шкале от 1 до 5. Использование LLM для валидации модели является хорошей практикой. Есть множество [статей](https://arxiv.org/abs/2401.07103), в которых доказывают, что LLM способны более точно оценивать сложные аспекты текста, такие как связность и релевантность контекста, хоть, кончено, это более затратно.

#### Llamaparse

Итак, давайте возьмем сырые документы и спарсим их с помощью [llamaparse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/). Полученные md файлы сохраним для скорости вычислений, да и потому что llamaparse позволяет бесплатно обрабатывать до 1000 страниц в день. Это достаточно много, однако это не позволяет нам заново парсить все документы каждый запуск модели.

#### База знаний

База знаний позволяет получать датасет из папки спаршенных документов.

#### QA генератор

Генератор состоит из, собственно, генератора и из критик-агента, который отсекает неуместные или некачественные вопросы, неверные или неполные ответы.

#### vectorstore

Спаршенные документы делятся на чанки, эти чанки с помощью токенизатора превращаются в вектора и попадают в векторную базу данных. Таким образом, в vectorstore (мы используем FAISS) находятся все ембеддинги.

#### Ретривер

Ретривер по запросу пользователя находит в базе данных те чанки, которые максимально похожи на заданный пользователем вопрос. Эти чанки могут очень сильно помочь, если мы передадим их в качестве контекста ридеру.

#### Ридер

Получает вопрос и информацию от ретривера и пытается ответить на вопрос.

#### Оценщик (valuator)

оценивает ответы модели на вопросы из синтетического набора данных

# Как воспроизвести эксперименты?

Чтобы проводить эксперементы с моделью, необходимо зайти в конфиг (config.yaml) и вставить туда [llamacloudAPI](https://cloud.llamaindex.ai/), [HFAPI](https://huggingface.co/settings/tokens) и [PineconeAPI](https://app.pinecone.io/organizations/-ODyMPMyGrGYdSCnYqSa/settings/projects). С конфигом можно эксперементировать. Например, если использовать реранкер, точноть модели будет выше. Далее необходимо запустить dashboard.py . Этот скрипт заупстит модель и валидацию, а позже покажет баллы всех сохраненных тестов, включая настоящий.

## Проведём эксперимент

Запустим валидацию для двух одинаковых конфигураций, только в одной будет реранкер, а в другой его не будет. Мы увидим, что оценка конфигурации с реранкером будет выше. 60% у конфига без реранкера против 75% с реранкером. Довольно сильная разница. Но почему так происходит?

./output\rag_chunk_200_embeddings_thenlper-gte-small_rerank_False_reader-model_mistralai-Mixtral-8x7B-Instruct-v0.1.json score = 60.0 %

./output\rag_chunk_200_embeddings_thenlper-gte-small_rerank_True_reader-model_mistralai-Mixtral-8x7B-Instruct-v0.1.json score = 75.0 %

## Что делает реранкер???

Используя глубокие нейронные сети, реранкеры глубже погружаются во взаимодействия между запросом и отобранными документами. Они выходят за рамки базовых документных эмбеддингов, охватывая токен-уровневые взаимодействия, происходящие внутри запроса, внутри документа и между запросом и документом. Хотя этот метод требует больше вычислительных ресурсов по сравнению с простым косинусным сходством, он позволяет выполнять нюансированное сравнение, учитывающее контекст, семантическое значение и намерение запроса, существенно улучшая релевантность результатов поиска.
Эффективность реранкеров так же подтверждается в [статьях](https://arxiv.org/pdf/2409.07691)

Таким образом, реранкеры могут сыграть значительную роль в улучшении нашей модели

## Что еще может повлиять на качество модели?

### Например, размер чанка.

Попробуем уменьшить чанк. С одной стороны, так как у нас модели с маленьким контекстом, нам может быть более выгодно уменьшить чанк. Но с другой стороны может попросту уменьшиться количество важной информации, которую мы передаем модели.
Предыдущие два эксперимента мы делали с размером чанка 200. Давайте попробуем с размером 100. (и включеным реранкером)

./output\rag_chunk_100_embeddings_thenlper-gte-small_rerank_True_reader-model_mistralai-Mixtral-8x7B-Instruct-v0.1.json score = 54.28571428571429 %

С большими чанками он вовсе начинает сходить с ума. Поэтому 200 - идеальное значение для размера чанка в нашей задаче

### Эксперименты c большими моделями

У меня нет возможности использовать более нагруженные модели, однако именно смена модели на более сильную может значительно улучшить правильность ответов.

# Тест модели

## Давайте опробуем нашу модель в деле. Для этого запустите скрипт use_model.py

### Несколько примеров ответов нашей модели, основанных на файлах в папке data//raw

> > > Что такое трансформер?
> > > Трансформер - это модель трансдукции, использующая исключительно самовнимание для вычисления представлений своего входного и выходного потока, без использования последовательно-выравненных RNN или конволюции.

> > > Что такое декодер?
> > > Декодер - это часть модели последовательностей, отвечающая за преобразование скрытых представлений в конечный выход. Он может содержать слои самовнимания, носительственные слои и выходной слой. Во время обучения, декодер получает предыдущие элементы последовательности, а в процессе предсказания - все предыдущие элементы, чтобы сгенерировать
> > > следующий элемент.

> > > Что такое енкодер?
> > > Энкодер — это компонент нейронной сети, отвечающий за преобразование входных данных в представление, которое затем может быть использовано другими частями модели. В контексте данного текста, энкодер использует механизм самовнимания (self-attention) в скрытом слое.

> > > В чем различие декодера и енкодера ?
> > > Энкодер и декодер - это два различных модуля в последовательностной трансдукции. Энкодер трансформирует входные данные в сокрытое представление, тогда как декодер использует это представление для предсказания выходных данных. Декодер также может использовать внимание, чтобы сосредоточиться на определенных частях входных данных.
